# Лекція 1

**Типи стиснення**:
- Без втрат

    Можна відтворити точно той файл, що був
- Зі втратами (не будемо вивчати)

    Вже не можна відтворити вихідний файл. Сенс в тому, щоб опустити те, що **людина** не сприймає
    - відео
    - фото


**Шеннон**
Запропонував формулу вимірювання кількості інформації

## Чому різні файли стискаються по різному?

Нехай $D_n = \{x_1...x_n : x_i = 0/1\}$. $|D_n| = 2^n$

Очевидно, не може бути бієкцій між $D_n$ та $D_m$. Тому однозначне відображення "стиснення" не може вміти стискати всі файли

На практиці деякі файли будуть роздуватися, а не стискатися. Наприклад, випадково згенеровані послідовності байтів.

## Інформація

> Кількість інформації - скільки невизначеності втрачено

**Приклади**:

1. **Лотерейний квиток**. 1 з 10000 виграшний. Ти не виграв - нічого дивного, інформації 0, все ще нічого не визначено
2. **Підкидання монети**. Кожне підкидання - багато інформації, бо кожне підкидання сильно зменшує невизначеність
3. В сумці 1 біла кулька. Ви вийняли білу кульку. Невизначеності було 0, стало 0, інформації 0
 

### Ентропія Шеннона
Нехай по інформаційному каналу передаються пакети над алфавітом в $n$ символів. Нехай відносні частоти кожного символа - $p_i$ 
$$H(p_1, ..., p_n), \quad p_i \in [0;1], \quad \sum_i p_i = 1$$

**Інтуітивні властивості ентропії**:
1. $H$ - гарно визначена та *неперервна*
2. $H(\frac{1}{n}, ..., \frac{1}{n}) < H(\frac{1}{n+1}, ..., \frac{1}{n+1}$, бо інтуітивно невизначеність в просторі з більшою кількостю різних подій має бути вища.
3. $$H(\frac{1}{n}, ..., \frac{1}{n}) = H(\frac{b_1}{n}, ..., \frac{b_n}{n}) + \sum_{i=1}^n \frac{b_i}{n} H(\underset{b_i}{\underbrace{\frac{1}{b_i}, ..., \frac{1}{b_i}}})$$

    Це еквівалентність двох експериментів. Ця формула пов'язує ентропію однорідної системи з ентропією неоднорідної.

    Ліворуч - $n$ різних кульок в одній коробці. Витягаємо одну випадкову кільку.

    Праворуч - $n$ різних кульок в $k$ коробках з різними розмірами $b_i$. $\sum_i b_i = n$. Вибираємо коробку (з ймовірностями, пропорційно їх розмірам), потім вибираємо кульку в обраній коробці. В результаті все одно витягнули одну випадкову кульку з усіх $n$ різних кульок.

> Шеннон показав, що існує **єдина** функція, що задовільняє всім трьом властивостям:

- **Ентропія Шеннона.**
    $$H(p_1, ..., p_n) = -K \sum_{i=1}^n p_i \log_2p_i$$ 
    $K$ - коефіцієнт для переходу між основами логарифма. Для двійкової ентропії $K=1$. Для десяткової - $K = \log_2 10$ тощо.

Мінус з'являється щоб ентропія була додатньою. Бо $p_i < 1 \implies \log p_i < 0$

- **Двійкова ентропія**
    $$-\sum_{i=1}^n p_i \log_2p_i$$

> $H$ показує: 
> * кількість інформації, яку несе кожен символ
> * кількість бітів, що необнідно для запису одного символа

Очікується (в 40х роках), що на символ алфавіту має припадати від $0.6$ до $1.3$ бітів

## Інший підхід до ентропії (Колмогоров)
Ентропія має залежати від затрат на алгоритм стиснення. Тобто чим складніше стискати набір даних - тим вище його ентропія

# Коди Хафмана

> Широко застосовуються, бо вони "майже оптимальні" та прості.

Нехай частоти символів в повідомленні - $p_1, ..., p_n$

### Приклад
Слово `ABRACADABRA`. Алфавіт - 5 символів. Довжина - 11.

Просте двійкове кодування - 3 біти. Тобто довжина повідомлення $11\cdot 3 = 33$ біти

Частоти літер:
`A = 5, B = R = 2, C = D = 1`

Спробуємо стиснути ефективніше:
```
A -> 0
B -> 10
R -> 110
C -> 1110
D -> 1111
```

Таким чином не буде невизначеності при читанні повідомлення. Не буде ситуації, коли не зрозуміло, це початок одного символа, чи кінець іншого.

В результаті, 23 біти!

> **Головне правило:** Частіші символи => Коротші коди